// trigger/etl-sync.ts
// Background job for ETL data synchronization
// Handles bulk data ingestion from external sources

import { task, wait } from "@trigger.dev/sdk/v3";

/**
 * ETL Sync Job Payload
 */
interface ETLSyncPayload {
    syncJobId: string;
    dataSourceId: string;
    sourceType: 'google_drive' | 'gmail' | 'notion' | 'slack' | 'dropbox' | 'onedrive';
    jobType: 'full_sync' | 'incremental' | 'delta' | 'manual';
    config: Record<string, any>;
    metadata: {
        userId: string;
        sourceName: string;
    };
}

/**
 * Main ETL Sync Job
 * 
 * Syncs data from external sources:
 * - Discovers items in source
 * - Detects changes (delta sync)
 * - Downloads new/modified items
 * - Processes and indexes documents
 */
export const etlSyncJob = task({
    id: "etl-sync",
    retry: {
        maxAttempts: 3,
        minTimeoutInMs: 5000,
        maxTimeoutInMs: 60000,
        factor: 2,
    },
    machine: {
        preset: "medium-1x",
    },
    run: async (payload: ETLSyncPayload, { ctx }) => {
        const { syncJobId, dataSourceId, sourceType, jobType, config, metadata } = payload;
        
        console.log(`[ETL Sync] ðŸš€ Starting ${jobType} sync for ${metadata.sourceName}`);
        console.log(`[ETL Sync] Source: ${sourceType}, Job: ${syncJobId}`);

        const baseUrl = process.env.NEXT_PUBLIC_APP_URL || 'http://localhost:3000';
        
        try {
            // 1. Update job status to running
            await updateSyncJob(baseUrl, syncJobId, {
                status: 'running',
                startedAt: new Date().toISOString(),
            });

            // 2. Discover items in source
            console.log(`[ETL Sync] ðŸ“‚ Discovering items...`);
            const items = await discoverItems(sourceType, config, metadata.userId);
            
            await updateSyncJob(baseUrl, syncJobId, {
                itemsFound: items.length,
            });

            console.log(`[ETL Sync] Found ${items.length} items`);

            // 3. Filter for changes (if delta sync)
            let itemsToProcess = items;
            if (jobType === 'delta' || jobType === 'incremental') {
                itemsToProcess = await filterChangedItems(baseUrl, dataSourceId, items);
                console.log(`[ETL Sync] ${itemsToProcess.length} items need processing`);
            }

            // 4. Process items in batches
            const batchSize = 5;
            let processed = 0;
            let created = 0;
            let updated = 0;
            let skipped = 0;
            let failed = 0;
            let bytesProcessed = 0;

            for (let i = 0; i < itemsToProcess.length; i += batchSize) {
                const batch = itemsToProcess.slice(i, i + batchSize);
                
                const results = await Promise.allSettled(
                    batch.map(item => processItem(baseUrl, item, dataSourceId, metadata.userId))
                );

                for (let j = 0; j < results.length; j++) {
                    const result = results[j];
                    const item = batch[j];

                    if (result.status === 'fulfilled') {
                        processed++;
                        bytesProcessed += item.fileSize || 0;
                        
                        if (result.value.action === 'created') created++;
                        else if (result.value.action === 'updated') updated++;
                        else if (result.value.action === 'skipped') skipped++;
                    } else {
                        failed++;
                        console.error(`[ETL Sync] âŒ Failed: ${item.name}: ${result.reason?.message}`);
                    }
                }

                // Update progress
                await updateSyncJob(baseUrl, syncJobId, {
                    itemsProcessed: processed,
                    itemsCreated: created,
                    itemsUpdated: updated,
                    itemsSkipped: skipped,
                    itemsFailed: failed,
                    bytesProcessed,
                });

                // Small delay between batches
                if (i + batchSize < itemsToProcess.length) {
                    await wait.for({ seconds: 1 });
                }
            }

            // 5. Complete job
            const finalStatus = failed === 0 ? 'completed' : 
                               failed === itemsToProcess.length ? 'failed' : 'completed';

            await updateSyncJob(baseUrl, syncJobId, {
                status: finalStatus,
                completedAt: new Date().toISOString(),
            });

            // 6. Update data source
            await updateDataSource(baseUrl, dataSourceId, {
                connectionStatus: 'connected',
                lastSyncAt: new Date().toISOString(),
                lastSyncStatus: finalStatus === 'completed' ? 'success' : 'partial',
                totalItemsSynced: processed,
            });

            console.log(`[ETL Sync] ðŸ Completed: ${processed} processed, ${created} created, ${updated} updated, ${failed} failed`);

            return {
                syncJobId,
                status: finalStatus,
                itemsFound: items.length,
                itemsProcessed: processed,
                itemsCreated: created,
                itemsUpdated: updated,
                itemsSkipped: skipped,
                itemsFailed: failed,
                bytesProcessed,
            };

        } catch (error: any) {
            console.error(`[ETL Sync] âŒ Job failed: ${error.message}`);

            await updateSyncJob(baseUrl, syncJobId, {
                status: 'failed',
                completedAt: new Date().toISOString(),
                errorMessage: error.message,
            });

            await updateDataSource(baseUrl, dataSourceId, {
                connectionStatus: 'error',
                lastSyncStatus: 'failed',
                lastSyncError: error.message,
            });

            throw error;
        }
    },
});

/**
 * Discover items in external source
 */
async function discoverItems(
    sourceType: string,
    config: Record<string, any>,
    userId: string
): Promise<Array<{
    externalId: string;
    name: string;
    path?: string;
    mimeType?: string;
    fileSize?: number;
    modifiedAt?: string;
    hash?: string;
}>> {
    // TODO: Implement actual source-specific discovery
    // This would use Composio tools or direct API calls
    
    switch (sourceType) {
        case 'google_drive':
            // Use Composio GOOGLEDRIVE_LIST_FILES
            return [];
            
        case 'gmail':
            // Use Composio GMAIL_LIST_MESSAGES
            return [];
            
        case 'notion':
            // Use Composio NOTION_SEARCH
            return [];
            
        case 'slack':
            // Use Composio SLACK_LIST_CHANNELS + messages
            return [];
            
        case 'dropbox':
            // Use Composio DROPBOX_LIST_FOLDER
            return [];
            
        case 'onedrive':
            // Use Microsoft Graph API
            return [];
            
        default:
            return [];
    }
}

/**
 * Filter items to only those that have changed
 */
async function filterChangedItems(
    baseUrl: string,
    dataSourceId: string,
    items: any[]
): Promise<any[]> {
    // TODO: Compare against sync_items table
    // Return only items with different hash or modifiedAt
    return items;
}

/**
 * Process a single item
 */
async function processItem(
    baseUrl: string,
    item: any,
    dataSourceId: string,
    userId: string
): Promise<{ action: 'created' | 'updated' | 'skipped' }> {
    // TODO: Implement actual item processing
    // 1. Download content
    // 2. Extract text
    // 3. Create/update document
    // 4. Generate embeddings
    // 5. Update sync_items record
    
    return { action: 'skipped' };
}

/**
 * Update sync job in database
 */
async function updateSyncJob(
    baseUrl: string,
    syncJobId: string,
    data: Record<string, any>
): Promise<void> {
    await fetch(`${baseUrl}/api/data-sources/sync/update`, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'x-internal-trigger': 'true',
        },
        body: JSON.stringify({
            syncJobId,
            ...data,
        }),
    });
}

/**
 * Update data source in database
 */
async function updateDataSource(
    baseUrl: string,
    dataSourceId: string,
    data: Record<string, any>
): Promise<void> {
    await fetch(`${baseUrl}/api/data-sources/update`, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'x-internal-trigger': 'true',
        },
        body: JSON.stringify({
            dataSourceId,
            ...data,
        }),
    });
}

/**
 * Google Drive specific sync job
 */
export const googleDriveSyncJob = task({
    id: "google-drive-sync",
    retry: { maxAttempts: 3 },
    run: async (payload: {
        syncJobId: string;
        dataSourceId: string;
        folderId?: string;
        userId: string;
    }) => {
        // TODO: Implement Google Drive specific sync
        // Uses Composio GOOGLEDRIVE_* tools
        console.log('[Google Drive Sync] Not yet implemented');
        return { status: 'skipped' };
    },
});

/**
 * Gmail sync job
 */
export const gmailSyncJob = task({
    id: "gmail-sync",
    retry: { maxAttempts: 3 },
    run: async (payload: {
        syncJobId: string;
        dataSourceId: string;
        query?: string;
        userId: string;
    }) => {
        // TODO: Implement Gmail specific sync
        // Uses Composio GMAIL_* tools
        console.log('[Gmail Sync] Not yet implemented');
        return { status: 'skipped' };
    },
});

/**
 * Notion sync job
 */
export const notionSyncJob = task({
    id: "notion-sync",
    retry: { maxAttempts: 3 },
    run: async (payload: {
        syncJobId: string;
        dataSourceId: string;
        workspaceId?: string;
        userId: string;
    }) => {
        // TODO: Implement Notion specific sync
        // Uses Composio NOTION_* tools
        console.log('[Notion Sync] Not yet implemented');
        return { status: 'skipped' };
    },
});
